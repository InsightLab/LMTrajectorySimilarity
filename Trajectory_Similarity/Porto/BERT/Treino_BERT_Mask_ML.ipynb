{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0zg18yzPSA6"
   },
   "source": [
    "## Treinando BERT do in√≠cio (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24040,
     "status": "ok",
     "timestamp": 1690417791043,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "E1M0EduoMRjI",
    "outputId": "c9e0bd29-c34e-4d8d-ab78-0ffefa3294dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /opt/tljh/user/lib/python3.7/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/tljh/user/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tokenizers in ./.local/lib/python3.7/site-packages (0.13.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.7/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/tljh/user/lib/python3.7/site-packages (from transformers) (6.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tljh/user/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.7/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tljh/user/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./.local/lib/python3.7/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.7/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.7/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.7/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/tljh/user/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.7/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.7/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install tokenizers\n",
    "!pip install transformers\n",
    "!pip install accelerate>=0.21.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, LineByLineTextDataset\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1690417796814,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "Lnf1RRlCmnJ7",
    "outputId": "91460327-8eae-4fb6-888b-25145eae7265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_p-pts.pickle\texp1-trj.h5\texp1-trj.t   README.md\t   train.trg  vocab.txt\r\n",
      "D_q-pts.pickle\texp1-trj.label\texp2-trj.h5  saved_models  val.src\r\n",
      "Dq-pts.pickle\texp1-trj.pts\tporto.csv    train.src\t   val.trg\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1690417805790,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "Rk0Ji_trXhoD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 18827\n"
     ]
    }
   ],
   "source": [
    "# Pegando o tamanho do vocabul√°rio:\n",
    "cels_list = []\n",
    "with open('../data/train.src') as f:\n",
    "  for line in f:\n",
    "    cels_traj = line.strip().split() # divide usando o espa√ßo como delimitador\n",
    "    \n",
    "    cels_traj = [int(cel) for cel in cels_traj]\n",
    "    cels_list.extend(cels_traj)\n",
    "\n",
    "\n",
    "size_vocab = len(set(cels_list))\n",
    "print('vocabulary size:', size_vocab)\n",
    "\n",
    "del cels_list, cels_traj  # liberando RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1690417817623,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "w9chmj0i_sp8",
    "outputId": "e9597e27-8d20-497b-ab96-ab1adea43e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506 112 144 148 250 258 384 106 15 4 71 1179 93 165 160 211 300 1245 547\r\n",
      "506 4846 506 112 144 148 250 258 384 106 15 4 1179 93 165 160 211 300 1245 547\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 '../data/train.src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77715,
     "status": "ok",
     "timestamp": 1690417897655,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "laUAO9KANt6W",
    "outputId": "93042c48-5b9a-4b3a-f625-f083ada88b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13369584 ../data/train.src\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l '../data/train.src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1690417999679,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "qsjfLgnalw5L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/vocab.txt']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinando o tokenizer:\n",
    "bwpt = tokenizers.BertWordPieceTokenizer(vocab=None)\n",
    "\n",
    "train_file = \"../data/train.src\"\n",
    "\n",
    "bwpt.train(\n",
    "    files=[train_file],\n",
    "    vocab_size=size_vocab,\n",
    "    min_frequency=1,\n",
    "    limit_alphabet=1000\n",
    ")\n",
    "\n",
    "bwpt.save_model('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['506', '112', '144', '148', '250', '258', '384']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-wilken.dantas@ufc.-af1ea/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1730: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "vocab_file_dir = '../data/vocab.txt'\n",
    "\n",
    "tokenizer =  BertTokenizer.from_pretrained(vocab_file_dir)\n",
    "\n",
    "sentence = '506 112 144 148 250 258 384'\n",
    "\n",
    "encoded_input = tokenizer.tokenize(sentence)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12576392,
     "status": "ok",
     "timestamp": 1690430600948,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "gSZNhwzrme82",
    "outputId": "b320561a-5500-4138-91ab-c7f1be15ed62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-wilken.dantas@ufc.-af1ea/.local/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:123: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of lines:  13369584\n",
      "CPU times: user 1h 17min 31s, sys: 14.5 s, total: 1h 17min 45s\n",
      "Wall time: 1h 17min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Carregando o df de treino\n",
    "'''\n",
    "transformers has a predefined class LineByLineTextDataset()\n",
    "which reads your text line by line and converts them to tokens\n",
    "'''\n",
    "\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = \"../data/train.src\",\n",
    "    block_size = 128  # maximum sequence length\n",
    ")\n",
    "\n",
    "print('No. of lines: ', len(train_dataset)) # No of lines in your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128207,
     "status": "ok",
     "timestamp": 1690430729144,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "nmI7JXuY0OJ8",
    "outputId": "83a721ad-9186-4e73-9c9c-830a3ac3297b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of lines:  134016\n",
      "CPU times: user 48.4 s, sys: 160 ms, total: 48.6 s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Carregando o df de valida√ß√£o\n",
    "'''\n",
    "transformers has a predefined class LineByLineTextDataset()\n",
    "which reads your text line by line and converts them to tokens\n",
    "'''\n",
    "\n",
    "eval_dataset = LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = \"../data/val.src\",\n",
    "    block_size = 128  # maximum sequence length\n",
    ")\n",
    "\n",
    "print('No. of lines: ', len(eval_dataset)) # No of lines in your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2177,
     "status": "ok",
     "timestamp": 1690430731313,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "e_9n6Xw9rBPF",
    "outputId": "b4753346-6628-4a4f-8378-f153c3fabcd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters:  220123531\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=size_vocab,\n",
    "    hidden_size=2048, # Dimensao do embedding (default --> 768). OBS: tem que ser um num m√∫ltimplo de \"num_attention_heads\"\n",
    "    num_hidden_layers=6, # 6\n",
    "    num_attention_heads=16, # 12\n",
    "    max_position_embeddings=512 # 1024\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "print('No of parameters: ', model.num_parameters())\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defindo os valores do \"early stopping\"\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # N√∫mero de avalia√ß√µes consecutivas sem melhora\n",
    "    early_stopping_threshold=0.02,  # Mudan√ßa m√≠nima na m√©trica para considerar melhora\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1690430731316,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "e54_1TxmuwJk"
   },
   "outputs": [],
   "source": [
    "# Configura√ß√µes de treino:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/saved_models/BERT/',\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4424,
     "status": "ok",
     "timestamp": 1690430735730,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "ycqP1S-Xvmlg"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[early_stopping_callback]  # Adicione o callback de \"early stopping\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "executionInfo": {
     "elapsed": 1509987,
     "status": "ok",
     "timestamp": 1690432250653,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "q0pF9FEBwHk1",
    "outputId": "61fe7760-318a-40ae-e01e-cdfcb05e1248"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-wilken.dantas@ufc.-af1ea/.local/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='4178000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  20000/4178000 2:53:17 < 600:30:59, 1.92 it/s, Epoch 0/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>8.411600</td>\n",
       "      <td>8.344678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>8.337200</td>\n",
       "      <td>8.326161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>8.328300</td>\n",
       "      <td>8.316226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>8.318900</td>\n",
       "      <td>8.310905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 53min 15s, sys: 11.7 s, total: 2h 53min 27s\n",
      "Wall time: 2h 53min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "trainer.save_model('../data/saved_models/BERT/best_model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMEsIrldIWoZvc5mVXOhKT6",
   "gpuClass": "premium",
   "machine_shape": "hm",
   "mount_file_id": "1TjHUjf0ytRcyM5tOuMgeRyaE15qimSec",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
