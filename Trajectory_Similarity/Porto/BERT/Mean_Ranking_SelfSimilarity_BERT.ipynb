{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20778,
     "status": "ok",
     "timestamp": 1694647048649,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "6jCyskjWe-Ii",
    "outputId": "01727a8a-89f3-4f49-f283-94540035a77c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14800,
     "status": "ok",
     "timestamp": 1694647063434,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "VP6Iw6AxfEcq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import scipy.stats as st\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1694647064403,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "Tndw-RF7cbbo",
    "outputId": "6a166462-bdbd-4f87-8288-dda5ca093a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_p-pts.pickle\texp1-trj.h5\texp1-trj.t   README.md\t   train.trg  vocab.txt\r\n",
      "D_q-pts.pickle\texp1-trj.label\texp2-trj.h5  saved_models  val.src\r\n",
      "Dq-pts.pickle\texp1-trj.pts\tporto.csv    train.src\t   val.trg\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1694647070290,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "lmGMWgthaaMY",
    "outputId": "eb810229-dfc2-4b62-ef90-a0b8ea85dfd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-wilken.dantas@ufc.-af1ea/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1730: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer of t2vec\n",
    "vocab_file_dir = '../data/vocab.txt'\n",
    "tokenizer =  BertTokenizer.from_pretrained(vocab_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1694647073974,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "qMyv5C-wYxtt",
    "outputId": "a89091ab-85ef-421e-d46f-321d05ca9e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model\t  checkpoint-15000  s1024  s64\r\n",
      "checkpoint-10000  checkpoint-20000  s256   s768\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../data/saved_models/BERT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2596,
     "status": "ok",
     "timestamp": 1694647138971,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "I76HDxeL3f4x",
    "outputId": "73e3d9b8-3357-432d-dbe1-6049a6a1ab0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../data/saved_models/BERT/best_model/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../data/saved_models/BERT/best_model/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Carregando o modelo treinado:\n",
    "config = BertConfig.from_json_file('../data/saved_models/BERT/best_model/config.json')\n",
    "config.output_hidden_states=True\n",
    "model = BertModel.from_pretrained('../data/saved_models/BERT/best_model/', local_files_only=True, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_mean_for_all_trajs(list_trajs):\n",
    "    tokenized_trajs = list_trajs\n",
    "    indexed_trajs_tokens = [tokenizer.convert_tokens_to_ids(traj) for traj in tokenized_trajs]\n",
    "\n",
    "    # Preenchendo as sequências para ter o mesmo comprimento (valor de preenchimento padrão = 0)\n",
    "    padded_inputs = rnn_utils.pad_sequence([torch.tensor(seq) for seq in indexed_trajs_tokens], batch_first=True)\n",
    "    #padded_inputs = padded_inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(padded_inputs)\n",
    "\n",
    "    # Calcula a média dos embeddings de cada sentença (traj)\n",
    "    sentence_embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = [['506', '112', '144', '148', '250', '258', '384'], ['148', '250', '258', '384']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5671,  1.0142,  0.4156,  ...,  0.9171, -0.3727,  1.4593],\n",
       "        [ 0.5671,  1.0142,  0.4156,  ...,  0.9171, -0.3727,  1.4593]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs = get_embedding_mean_for_all_trajs(trajectories)\n",
    "vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2048])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensão dos embeddings\n",
    "dim = vecs.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508 465 1641 857 3176 1346 1301 3303 3277 3977 4430 8513 9755 11383 9496 12228 11150 13279 9215 17279 14428 9279 14792 14310 18351 7997 15024 15267 15665 16329 15125 14591 14797 3\r\n",
      "19 191 68 41 46 4 964 543 154 171 382 732 632 923 4815 460 273 439 607 908 726 6277 5338 5821 6933 5438 8005 10438 9812 17030 12351 12132 13988 9320 5581 13142 15720 14944 5048 11012\r\n",
      "51 2263 345 53 120 405 803 585 692 1566 533 1728 1880 739 544 226 8\r\n",
      "15 4 58 9 38 54 196 648 193 1022 575 1560 753 229 260 532 317 764 159 633 1304 207 798 2811 301 211 880 80 23 89 40 537 781 141 5900 240 38 54 655 1616 196 648 437 749 575\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 '../data/exp1-trj.t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 6573,
     "status": "ok",
     "timestamp": 1694647179149,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "LodboHE6z2R0"
   },
   "outputs": [],
   "source": [
    "# Carregando as trajs de teste:\n",
    "trajs_teste = []\n",
    "with open('../data/exp1-trj.t') as f:\n",
    "    for line in f:\n",
    "        traj_list = line.strip().split()\n",
    "        trajs_teste.append(traj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1694647179150,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "bpT-8Hcxz2G2",
    "outputId": "49c66124-1053-4ef8-8473-e000178e14b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de trajetórias de teste: 101000\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de trajetórias de teste:\", len(trajs_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['508', '465', '1641', '857', '3176', '1346', '1301', '3303', '3277', '3977', '4430', '8513', '9755', '11383', '9496', '12228', '11150', '13279', '9215', '17279', '14428', '9279', '14792', '14310', '18351', '7997', '15024', '15267', '15665', '16329', '15125', '14591', '14797', '3']\n"
     ]
    }
   ],
   "source": [
    "print(trajs_teste[0]) # --> primeira query (traj par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1089', '465', '1123', '2173', '185', '307', '553', '4012', '3442', '4296', '4498', '7490', '7819', '7630', '9013', '11297', '11988', '10689', '18272', '12037', '11976', '15001', '15262', '15156', '14901', '7997', '13957', '16913', '16146', '15069', '15704', '16382', '16121', '16271', '3']\n"
     ]
    }
   ],
   "source": [
    "print(trajs_teste[1000]) # --> mais semelhante a primeira query, ou seja, traj ímpar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1694647188024,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "fkv5lR35nvNq"
   },
   "outputs": [],
   "source": [
    "# Segmentando: query (trajs pares) e dbsearch (querys ímpar + 99000 outras ímpares)\n",
    "query = trajs_teste[:1000] # trajs query (pares)\n",
    "dbsearch = trajs_teste[1000:101000] # dbsearch trajs (as 1000 primeiras são as query ímpar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(query))\n",
    "print(len(dbsearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abaixo, pegamos os embeddings das trajs por lotes (10 em 10 trajs). Assim, evita-se estouro de memória..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 57s, sys: 3.53 s, total: 20min 1s\n",
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query_aux = torch.zeros(1000, dim)\n",
    "\n",
    "for i in range(0, len(query), 10):\n",
    "    query_aux[i:i+10] = get_embedding_mean_for_all_trajs(query[i:i+10]) # Pegando de 10 em 10\n",
    "    \n",
    "    # Calcula a porcentagem concluída\n",
    "    percent_done = ((i / len(query)) * 100)+1\n",
    "    print(f\"Progresso: {percent_done:.2f}% concluído\", end=\"\\r\")  # A opção `end=\"\\r\"` permite que a impressão seja substituída na mesma linha\n",
    "    \n",
    "query = query_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progresso: 50.86% concluído\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dbsearch_aux = torch.zeros(100000, dim)\n",
    "\n",
    "for i in range(0, len(dbsearch), 10):\n",
    "    dbsearch_aux[i:i+10] = get_embedding_mean_for_all_trajs(dbsearch[i:i+10]) # Pegando de 10 em 10\n",
    "    \n",
    "    # Calcula a porcentagem concluída\n",
    "    percent_done = ((i / len(dbsearch)) * 100)+1\n",
    "    print(f\"Progresso: {percent_done:.2f}% concluído\", end=\"\\r\")  # A opção `end=\"\\r\"` permite que a impressão seja substituída na mesma linha\n",
    "    \n",
    "dbsearch = dbsearch_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(t_query, i, db_search): \n",
    "    dists = cosine_distances(t_query.reshape(1, -1), db_search) # pega todas as distâncias de Dq[i] as trajs do db_search (D_qUD_p)\n",
    "    dists = dists.flatten()\n",
    "    order = dists.argsort() # pega a ordem\n",
    "    ranks = order.argsort() # pega o rank\n",
    "\n",
    "    return ranks[i] + 1 # retorna o ranking de ta' no dbsearch | soma +1 pq o argsort rankea a partir de 0 (zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(ranks):\n",
    "    count = 0\n",
    "    for i in range(len(ranks)):\n",
    "        if ranks[i] == 1:\n",
    "            count += 1\n",
    "\n",
    "    return round(count/len(ranks), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr(ranks):\n",
    "    return (sum(ranks)/len(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYObsBJvjxeT"
   },
   "outputs": [],
   "source": [
    "def mrr(ranks):\n",
    "    count = 0\n",
    "    for i in range(len(ranks)):\n",
    "        count += 1/ranks[i]\n",
    "\n",
    "    return round(count/len(ranks), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervalo de Confiança do Ranks\n",
    "def cip_r(ranks):\n",
    "    data = []\n",
    "    for i in range(len(ranks)):\n",
    "        data.append(ranks[i]) # Add os Ranks\n",
    "    \n",
    "    #create 95% confidence interval for population mean weight\n",
    "    ic = st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data))\n",
    "    \n",
    "    return tuple(round(valor, 3) for valor in ic) # arredonda pra 3 casas decimais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervalo de Confiança dos Reciprocal Ranks\n",
    "def cip_rr(ranks):\n",
    "    data = []\n",
    "    for i in range(len(ranks)):\n",
    "        data.append(1/ranks[i]) # Add os RRs...\n",
    "    \n",
    "    #create 95% confidence interval for population mean weight\n",
    "    ic = st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data))\n",
    "    \n",
    "    return tuple(round(valor, 3) for valor in ic) # arredonda pra 3 casas decimais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPlYZOevrR4"
   },
   "source": [
    "## Usando Discretização de Cels do t2vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPlYZOevrR4"
   },
   "source": [
    "### BertConfig():\n",
    "    hidden_size=2048,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=16,\n",
    "    max_position_embeddings=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# bert model ep1, s2048, best_model\n",
    "dbsizes = [20000, 40000, 60000, 80000, 100000]\n",
    "for dbsize in dbsizes:\n",
    "    ranks = []\n",
    "    search = dbsearch[:dbsize]\n",
    "    for i in range(len(query)):\n",
    "        ranks.append(rank(query[i], i, search)) # rank das ta em fatias do dbsearch!\n",
    "    print('Mean rank: {}{}, Acc: {}, MRR: {}{} with dbsearch size: {}'.format(mr(ranks), cip_r(ranks), acc(ranks), mrr(ranks), cip_rr(ranks), dbsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPlYZOevrR4"
   },
   "source": [
    "### BertConfig():\n",
    "    hidden_size=1024,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=16,\n",
    "    max_position_embeddings=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rank: 1.408(1.217, 1.599), Acc: 0.94, MRR: 0.96(0.947, 0.969) with dbsearch size: 20000\n",
      "Mean rank: 1.784(1.409, 2.159), Acc: 0.91, MRR: 0.94(0.927, 0.952) with dbsearch size: 40000\n",
      "Mean rank: 2.174(1.598, 2.75), Acc: 0.9, MRR: 0.93(0.915, 0.942) with dbsearch size: 60000\n",
      "Mean rank: 2.629(1.83, 3.428), Acc: 0.88, MRR: 0.92(0.901, 0.931) with dbsearch size: 80000\n",
      "Mean rank: 2.963(2.011, 3.915), Acc: 0.86, MRR: 0.91(0.891, 0.922) with dbsearch size: 100000\n",
      "CPU times: user 30min 21s, sys: 38min 11s, total: 1h 8min 33s\n",
      "Wall time: 21min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bert model ep1, s1024, best_model\n",
    "dbsizes = [20000, 40000, 60000, 80000, 100000]\n",
    "for dbsize in dbsizes:\n",
    "    ranks = []\n",
    "    search = dbsearch[:dbsize]\n",
    "    for i in range(len(query)):\n",
    "        ranks.append(rank(query[i], i, search)) # rank das ta em fatias do dbsearch!\n",
    "    print('Mean rank: {}{}, Acc: {}, MRR: {}{} with dbsearch size: {}'.format(mr(ranks), cip_r(ranks), acc(ranks), mrr(ranks), cip_rr(ranks), dbsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPlYZOevrR4"
   },
   "source": [
    "### BertConfig():\n",
    "    hidden_size=64,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    max_position_embeddings=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rank: 1.851(1.538, 2.164), Acc: 0.86, MRR: 0.91(0.894, 0.924) with dbsearch size: 20000\n",
      "Mean rank: 2.618(2.043, 3.193), Acc: 0.82, MRR: 0.87(0.856, 0.891) with dbsearch size: 40000\n",
      "Mean rank: 3.361(2.529, 4.193), Acc: 0.8, MRR: 0.85(0.834, 0.872) with dbsearch size: 60000\n",
      "Mean rank: 4.241(3.105, 5.377), Acc: 0.77, MRR: 0.83(0.812, 0.852) with dbsearch size: 80000\n",
      "Mean rank: 5.046(3.626, 6.466), Acc: 0.76, MRR: 0.82(0.801, 0.842) with dbsearch size: 100000\n",
      "CPU times: user 11min 4s, sys: 29min 23s, total: 40min 28s\n",
      "Wall time: 10min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bert model ep1, s64, best_model\n",
    "dbsizes = [20000, 40000, 60000, 80000, 100000]\n",
    "for dbsize in dbsizes:\n",
    "    ranks = []\n",
    "    search = dbsearch[:dbsize]\n",
    "    for i in range(len(query)):\n",
    "        ranks.append(rank(query[i], i, search)) # rank das ta em fatias do dbsearch!\n",
    "    print('Mean rank: {}{}, Acc: {}, MRR: {}{} with dbsearch size: {}'.format(mr(ranks), cip_r(ranks), acc(ranks), mrr(ranks), cip_rr(ranks), dbsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPlYZOevrR4"
   },
   "source": [
    "### BertConfig():\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=16,\n",
    "    max_position_embeddings=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rank: 1.957(1.285, 2.629), Acc: 0.9, MRR: 0.93(0.917, 0.944) with dbsearch size: 20000\n",
      "Mean rank: 2.827(1.526, 4.128), Acc: 0.88, MRR: 0.91(0.896, 0.927) with dbsearch size: 40000\n",
      "Mean rank: 3.643(1.767, 5.519), Acc: 0.86, MRR: 0.9(0.882, 0.915) with dbsearch size: 60000\n",
      "Mean rank: 4.664(2.08, 7.248), Acc: 0.83, MRR: 0.88(0.861, 0.897) with dbsearch size: 80000\n",
      "Mean rank: 5.615(2.312, 8.918), Acc: 0.82, MRR: 0.87(0.848, 0.884) with dbsearch size: 100000\n",
      "CPU times: user 16min 6s, sys: 37min 23s, total: 53min 29s\n",
      "Wall time: 13min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bert model ep1, s256, best_model\n",
    "dbsizes = [20000, 40000, 60000, 80000, 100000]\n",
    "for dbsize in dbsizes:\n",
    "    ranks = []\n",
    "    search = dbsearch[:dbsize]\n",
    "    for i in range(len(query)):\n",
    "        ranks.append(rank(query[i], i, search)) # rank das ta em fatias do dbsearch!\n",
    "    print('Mean rank: {}{}, Acc: {}, MRR: {}{} with dbsearch size: {}'.format(mr(ranks), cip_r(ranks), acc(ranks), mrr(ranks), cip_rr(ranks), dbsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPlYZOevrR4"
   },
   "source": [
    "### BertConfig():\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rank: 1.47(1.283, 1.657), Acc: 0.92, MRR: 0.94(0.933, 0.957) with dbsearch size: 20000\n",
      "Mean rank: 1.872(1.531, 2.213), Acc: 0.88, MRR: 0.92(0.907, 0.935) with dbsearch size: 40000\n",
      "Mean rank: 2.276(1.763, 2.789), Acc: 0.87, MRR: 0.91(0.895, 0.925) with dbsearch size: 60000\n",
      "Mean rank: 2.779(2.073, 3.485), Acc: 0.85, MRR: 0.9(0.881, 0.913) with dbsearch size: 80000\n",
      "Mean rank: 3.162(2.31, 4.014), Acc: 0.84, MRR: 0.89(0.871, 0.904) with dbsearch size: 100000\n",
      "CPU times: user 24min 42s, sys: 37min 41s, total: 1h 2min 24s\n",
      "Wall time: 18min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bert model ep1, s768, best_model\n",
    "dbsizes = [20000, 40000, 60000, 80000, 100000]\n",
    "for dbsize in dbsizes:\n",
    "    ranks = []\n",
    "    search = dbsearch[:dbsize]\n",
    "    for i in range(len(query)):\n",
    "        ranks.append(rank(query[i], i, search)) # rank das ta em fatias do dbsearch!\n",
    "    print('Mean rank: {}{}, Acc: {}, MRR: {}{} with dbsearch size: {}'.format(mr(ranks), cip_r(ranks), acc(ranks), mrr(ranks), cip_rr(ranks), dbsize))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNb7ADocPT+4khlxLjiFzWx",
   "collapsed_sections": [
    "3CNDIKIh5PxT"
   ],
   "machine_shape": "hm",
   "mount_file_id": "180jxnrEfenoBKq14C11iOaIFCq1ZhZJv",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
