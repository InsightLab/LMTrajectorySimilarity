{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20778,
     "status": "ok",
     "timestamp": 1694647048649,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "6jCyskjWe-Ii",
    "outputId": "01727a8a-89f3-4f49-f283-94540035a77c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14800,
     "status": "ok",
     "timestamp": 1694647063434,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "VP6Iw6AxfEcq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import scipy.stats as st\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1694647064403,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "Tndw-RF7cbbo",
    "outputId": "6a166462-bdbd-4f87-8288-dda5ca093a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tdrive_ALL-taxis.csv  exp1-trj.h5\t tdrive.csv\t\t   val2.src\r\n",
      "D_p-pts.pickle\t\t exp1-trj.label  tdrive_formato_t2vec.csv  val.mta\r\n",
      "D_q-pts.pickle\t\t exp1-trj.t\t train.mta\t\t   val.src\r\n",
      "Dq-pts.pickle\t\t mv.csv\t\t train.src\t\t   val.trg\r\n",
      "exp1-querydb.h5\t\t saved_models\t train.trg\t\t   vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1694647070290,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "lmGMWgthaaMY",
    "outputId": "eb810229-dfc2-4b62-ef90-a0b8ea85dfd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-wilken.dantas@ufc.-af1ea/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1730: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer of t2vec\n",
    "vocab_file_dir = '../data/vocab.txt'\n",
    "tokenizer =  BertTokenizer.from_pretrained(vocab_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1694647073974,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "qMyv5C-wYxtt",
    "outputId": "a89091ab-85ef-421e-d46f-321d05ca9e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model\t   checkpoint-140000  checkpoint-150000  tmp\r\n",
      "checkpoint-135000  checkpoint-145000  checkpoint-155000\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../data/saved_models/BERT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2596,
     "status": "ok",
     "timestamp": 1694647138971,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "I76HDxeL3f4x",
    "outputId": "73e3d9b8-3357-432d-dbe1-6049a6a1ab0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../data/saved_models/BERT/best_model/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../data/saved_models/BERT/best_model/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Carregando o modelo treinado:\n",
    "config = BertConfig.from_json_file('../data/saved_models/BERT/best_model/config.json')\n",
    "config.output_hidden_states=True\n",
    "model = BertModel.from_pretrained('../data/saved_models/BERT/best_model/', local_files_only=True, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_mean_for_all_trajs(list_trajs):\n",
    "    tokenized_trajs = list_trajs\n",
    "    indexed_trajs_tokens = [tokenizer.convert_tokens_to_ids(traj) for traj in tokenized_trajs]\n",
    "\n",
    "    # Preenchendo as sequências para ter o mesmo comprimento (valor de preenchimento padrão = 0)\n",
    "    padded_inputs = rnn_utils.pad_sequence([torch.tensor(seq) for seq in indexed_trajs_tokens], batch_first=True)\n",
    "    #padded_inputs = padded_inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(padded_inputs)\n",
    "\n",
    "    # Calcula a média dos embeddings de cada sentença (traj)\n",
    "    sentence_embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = [['17321', '10721', '17321', '10721', '11693'], ['17321', '10721', '11693']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5071,  0.0927,  0.8889,  ..., -0.1824,  0.5921, -0.5153],\n",
       "        [ 0.9438,  0.2829,  0.3659,  ..., -0.5627,  0.6390, -0.6088]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = get_embedding_mean_for_all_trajs(trajectories)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = embs.shape[1]\n",
    "emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\r\n",
      "3\r\n",
      "3\r\n",
      "3\r\n",
      "3\r\n",
      "3\r\n",
      "3\r\n",
      "3\r\n",
      "3\r\n",
      "3\r\n",
      "9926 3189 88 7163 4076 13490 19059 13124 7310 5479\r\n",
      "9042 3107 4616 5742 4717 6339 15188 18899 14919 7274 11265 1081 7609 1351 393\r\n",
      "427 5334 8388 8967 4445 2056\r\n",
      "542 4013 17293 3322 11357 483 11138\r\n",
      "481 8071 657 2231 1769 173 1579 6198\r\n",
      "854 3107 5989 8778 6218 7338\r\n",
      "5773 8574 13056 11926 15551 6461\r\n",
      "4134 3764 9341 15412 13024 9086\r\n",
      "962 9873 469 373 6096 567 8620\r\n",
      "14405 4076 5305 3107 6807 1155\r\n",
      "4913 3694 3340 5104 95\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 21 '../data/exp1-trj.t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 6573,
     "status": "ok",
     "timestamp": 1694647179149,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "LodboHE6z2R0"
   },
   "outputs": [],
   "source": [
    "# Carregando as trajs de teste:\n",
    "trajs_teste = []\n",
    "with open('../data/exp1-trj.t') as f:\n",
    "    for line in f:\n",
    "        traj_list = line.strip().split()\n",
    "        trajs_teste.append(traj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1694647179150,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "bpT-8Hcxz2G2",
    "outputId": "49c66124-1053-4ef8-8473-e000178e14b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de trajetórias de teste: 50500\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de trajetórias de teste:\", len(trajs_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9926', '3189', '88', '7163', '4076', '13490', '19059', '13124', '7310', '5479']\n"
     ]
    }
   ],
   "source": [
    "print(trajs_teste[10]) # --> primeira query (traj par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9926', '8231', '92', '12594', '14968', '11048', '7821', '577']\n"
     ]
    }
   ],
   "source": [
    "print(trajs_teste[510]) # --> mais semelhante a primeira query, ou seja, traj ímpar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1694647188024,
     "user": {
      "displayName": "Wilken Charles",
      "userId": "08238201343018523362"
     },
     "user_tz": 180
    },
    "id": "fkv5lR35nvNq"
   },
   "outputs": [],
   "source": [
    "# Segmentando: query (trajs pares) e dbsearch (querys ímpar + 49500 outras ímpares)\n",
    "query = trajs_teste[:500] # trajs query (pares)\n",
    "dbsearch = trajs_teste[500:50500] # dbsearch trajs (as 500 primeiras são as query ímpar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(query))\n",
    "print(len(dbsearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abaixo, pegamos os embeddings das trajs por lotes (10 em 10 trajs). Assim, evita-se estouro de memória..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 22s, sys: 48.2 ms, total: 2min 22s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query_aux = torch.zeros(len(query), emb_dim)\n",
    "\n",
    "for i in range(0, len(query), 10):\n",
    "    query_aux[i:i+10] = get_embedding_mean_for_all_trajs(query[i:i+10]) # Pegando de 10 em 10\n",
    "    \n",
    "    # Calcula a porcentagem concluída\n",
    "    percent_done = ((i / len(query)) * 100)+1\n",
    "    print(f\"Progresso: {percent_done:.2f}% concluído\", end=\"\\r\")  # A opção `end=\"\\r\"` permite que a impressão seja substituída na mesma linha\n",
    "    \n",
    "query = query_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 37min 28s, sys: 7.5 s, total: 3h 37min 36s\n",
      "Wall time: 55min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dbsearch_aux = torch.zeros(len(dbsearch), emb_dim)\n",
    "\n",
    "for i in range(0, len(dbsearch), 10):\n",
    "    dbsearch_aux[i:i+10] = get_embedding_mean_for_all_trajs(dbsearch[i:i+10]) # Pegando de 10 em 10\n",
    "    \n",
    "    # Calcula a porcentagem concluída\n",
    "    percent_done = ((i / len(dbsearch)) * 100)+1\n",
    "    print(f\"Progresso: {percent_done:.2f}% concluído\", end=\"\\r\")  # A opção `end=\"\\r\"` permite que a impressão seja substituída na mesma linha\n",
    "    \n",
    "dbsearch = dbsearch_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(t_query, i, db_search): \n",
    "    dists = cosine_distances(t_query.reshape(1, -1), db_search) # pega todas as distâncias de Dq[i] as trajs do db_search (D_qUD_p)\n",
    "    dists = dists.flatten()\n",
    "    order = dists.argsort() # pega a ordem\n",
    "    ranks = order.argsort() # pega o rank\n",
    "\n",
    "    return ranks[i] + 1 # retorna o ranking de ta' no dbsearch | soma +1 pq o argsort rankea a partir de 0 (zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(ranks):\n",
    "    count = 0\n",
    "    for i in range(len(ranks)):\n",
    "        if ranks[i] == 1:\n",
    "            count += 1\n",
    "\n",
    "    return round(count/len(ranks), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr(ranks):\n",
    "    return (sum(ranks)/len(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KYObsBJvjxeT"
   },
   "outputs": [],
   "source": [
    "def mrr(ranks):\n",
    "    count = 0\n",
    "    for i in range(len(ranks)):\n",
    "        count += 1/ranks[i]\n",
    "\n",
    "    return round(count/len(ranks), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervalo de Confiança do Ranks\n",
    "def cip_r(ranks):\n",
    "    data = []\n",
    "    for i in range(len(ranks)):\n",
    "        data.append(ranks[i]) # Add os Ranks\n",
    "    \n",
    "    #create 95% confidence interval for population mean weight\n",
    "    ic = st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data))\n",
    "    \n",
    "    return tuple(round(valor, 3) for valor in ic) # arredonda pra 3 casas decimais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervalo de Confiança dos Reciprocal Ranks\n",
    "def cip_rr(ranks):\n",
    "    data = []\n",
    "    for i in range(len(ranks)):\n",
    "        data.append(1/ranks[i]) # Add os RRs...\n",
    "    \n",
    "    #create 95% confidence interval for population mean weight\n",
    "    ic = st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data))\n",
    "    \n",
    "    return tuple(round(valor, 3) for valor in ic) # arredonda pra 3 casas decimais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPlYZOevrR4"
   },
   "source": [
    "## Usando Discretização de Cels do t2vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VPlYZOevrR4"
   },
   "source": [
    "### BertConfig():\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rank: 83.344(51.105, 115.583), Acc: 0.66, MRR: 0.72(0.688, 0.759) with dbsearch size: 10000\n",
      "Mean rank: 151.736(92.822, 210.65), Acc: 0.65, MRR: 0.7(0.668, 0.741) with dbsearch size: 20000\n",
      "Mean rank: 207.646(124.241, 291.051), Acc: 0.65, MRR: 0.7(0.66, 0.735) with dbsearch size: 30000\n",
      "Mean rank: 262.304(157.7, 366.908), Acc: 0.64, MRR: 0.69(0.649, 0.724) with dbsearch size: 40000\n",
      "Mean rank: 345.338(205.169, 485.507), Acc: 0.62, MRR: 0.68(0.64, 0.715) with dbsearch size: 50000\n",
      "CPU times: user 11min 5s, sys: 16min 50s, total: 27min 56s\n",
      "Wall time: 7min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ep1, s768, best_model\n",
    "dbsizes = [10000, 20000, 30000, 40000, 50000]\n",
    "for dbsize in dbsizes:\n",
    "    ranks = []\n",
    "    search = dbsearch[:dbsize]\n",
    "    for i in range(len(query)):\n",
    "        ranks.append(rank(query[i], i, search)) # rank das ta em fatias do dbsearch!\n",
    "    print('Mean rank: {}{}, Acc: {}, MRR: {}{} with dbsearch size: {}'.format(mr(ranks), cip_r(ranks), acc(ranks), mrr(ranks), cip_rr(ranks), dbsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rank: 96.698(51.093, 142.303), Acc: 0.64, MRR: 0.71(0.671, 0.743) with dbsearch size: 10000\n",
      "Mean rank: 180.668(92.038, 269.298), Acc: 0.63, MRR: 0.69(0.653, 0.726) with dbsearch size: 20000\n",
      "Mean rank: 256.87(126.934, 386.806), Acc: 0.61, MRR: 0.67(0.632, 0.707) with dbsearch size: 30000\n",
      "Mean rank: 328.794(157.884, 499.704), Acc: 0.6, MRR: 0.66(0.621, 0.697) with dbsearch size: 40000\n",
      "Mean rank: 417.794(201.379, 634.209), Acc: 0.59, MRR: 0.65(0.613, 0.69) with dbsearch size: 50000\n",
      "CPU times: user 11min 31s, sys: 17min, total: 28min 31s\n",
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ep1, s768, checkpoint-110000\n",
    "dbsizes = [10000, 20000, 30000, 40000, 50000]\n",
    "for dbsize in dbsizes:\n",
    "    ranks = []\n",
    "    search = dbsearch[:dbsize]\n",
    "    for i in range(len(query)):\n",
    "        ranks.append(rank(query[i], i, search)) # rank das ta em fatias do dbsearch!\n",
    "    print('Mean rank: {}{}, Acc: {}, MRR: {}{} with dbsearch size: {}'.format(mr(ranks), cip_r(ranks), acc(ranks), mrr(ranks), cip_rr(ranks), dbsize))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNb7ADocPT+4khlxLjiFzWx",
   "collapsed_sections": [
    "3CNDIKIh5PxT"
   ],
   "machine_shape": "hm",
   "mount_file_id": "180jxnrEfenoBKq14C11iOaIFCq1ZhZJv",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
