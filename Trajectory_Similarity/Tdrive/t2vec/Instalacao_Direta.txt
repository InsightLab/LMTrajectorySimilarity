############### Usando o Server Cascavel - Quixadá ################

<<<<>>>> PASSOS USADOS PARA TREINO & TESTE DO MODELO T2VEC <<<<>>>>

1°. Liste os envs:
$ conda env list

2°. Crie um novo env (tdrive) clonando o env base:
$ conda create --name tdrive --clone base
Obs: Isto foi necessário para que o novo env carregasse os drivers corretos do Pytorch + CUDA!

3°. Verifique se o env tdrive foi realmente criado:
$ conda env list

4°. Ative o env:
$ conda activate tdrive

5°. Verifique se o Pytorch e CUDA estão funcionando corretamente:
$ python
>>> import torch
>>> print(torch.__version__)
1.13.1+cu117
>>> torch.cuda.is_available()
True
>>> exit()
Obs: Se a saída for "True", significa que sim!

6°. Instale a versão 1.18.5 do Numpy para evitar os erros de "inhomogeneous part" ou "VisibleDeprecationWarning":
$ conda install numpy=1.18.5
$ python
>>> import numpy as np
>>> print(np.__version__)
1.18.5

7°. Verifique se a linguem Julia está instalada:
$ julia
Caso não, instale a linguagem Julia! Pode ser uma versão superior à 1.5, e.x:
$ conda install -c conda-forge julia=1.6.3
Ou, então, instale a versão 1.5 manualmente.

8°. Clone o projeto t2vec
$ cd ~/.conda/envs/tdrive
$ git clone https://github.com/boathit/t2vec.git
$ ls
$ cd t2vec/data/

9°. Copie o dataset T-drive no formato t2vec para dentro de data/:
$ cp -fv ~/TrajectoryEmbeddings/Tdrive/data/tdrive_formato_t2vec.csv .
$ mv tdrive_formato_t2vec.csv tdrive.csv

10°. Faça as seguintes modificações de arquivos:
$ cd ..
a) Em pkg-install.jl:
Comente --> Pkg.add("IJulia") --> #Pkg.add("IJulia")
Adicione a linha --> Pkg.add("ArgParse")
$ cd preprocessing/

b) Em porto2h5.jl
Atualize --> default="/home/xiucheng/Github/t2vec/data" Para --> default="/home/jupyter-wilken.dantas@ufc.-af1ea/.conda/envs/tdrive/t2vec/data"

c) Em preprocess.jl:
Atualize --> default="/home/xiucheng/Github/t2vec/data" Para --> default="/home/jupyter-wilken.dantas@ufc.-af1ea/.conda/envs/tdrive/t2vec/data"

d) Em utils.jl, linha 121:
Atualize --> dropping_rates = [0, 0.2, 0.4, 0.5, 0.6] Para --> dropping_rates = [0, 0.2, 0.4, 0.6]
Obs: Etapa importante, pois formará somente os 16 pares de treino, como dito no artigo e não feito no código deles!
$ cd ..

e) Em t2vec.py:
linha 18:
Atualize --> parser.add_argument("-data", default="/home/xiucheng/Github/t2vec/data", help="Path to training and validating data") Para --> parser.add_argument("-data", default="/home/jupyter-wilken.dantas@ufc.-af1ea/.conda/envs/tdrive/t2vec/data", help="Path to training and validating data")

linha 21: 
Atualize --> parser.add_argument("-checkpoint", default="/home/xiucheng/Github/t2vec/data/checkpoint.pt", help="The saved checkpoint") Para --> parser.add_argument("-checkpoint", default="/home/jupyter-wilken.dantas@ufc.-af1ea/.conda/envs/tdrive/t2vec/data/checkpoint.pt", help="The saved checkpoint")

11°. Instale as seguintes bibliotecas:
$ pip install funcy
$ pip install h5py

12°. Instale os pacotes adicionais Julia:
$ julia pkg-install.jl

13°. Execute os pré-processamentos:
$ cd preprocessing/
$ mv porto2h5.jl tdrive2h5.jl

14°. Em tdrive2h5.jl:
linha 16: 
Atualize: porto2h5("$datapath/porto.csv") -->  porto2h5("$datapath/tdrive.csv")

15°. Em utils.jl
linha 15: 
Comente: sort!(df, [:TIMESTAMP])  -->  #sort!(df, [:TIMESTAMP])
linha 18: 
Atualize: h5open("../data/porto.h5", "w")  -->  h5open("../data/tdrive.h5", "w")
linha 32: 
Atualize: f["/timestamps/$num"] = collect(0:tripLength-1) * 15.0  -->   f["/timestamps/$num"] = collect(0:tripLength-1) * 187.0

16°. Atualize o hyper-parameters.json para a nova região tdrive (cidade Pequim): 
$ cd ..
$ vim hyper-parameters.json

De:
{  "region":  
  { 
    "cityname":"porto",    
    "minlon":-8.735152,
    "minlat":40.953673,
    "maxlon":-8.156309,
    "maxlat":41.307945,
    "cellsize":100.0,
    "minfreq":100
  }
}

Para:
{  "region":  
  { 
    "cityname":"tdrive",    
    "minlon":116.09613569127211,
    "minlat":39.72901776500179,
    "maxlon":116.68168430872788,
    "maxlat":40.0883422349982,
    "cellsize":100.0,
    "minfreq":100
  }
}

$ cd preprocessing/

17°. Em SpatialRegionTools.jl
linha 345:
Atualize: min_length=20  --> min_length=5
linha 346:
Atualize: max_length=100  --> max_length=50 

18°. - Em preprocess.jl
linha 60:
Atualize: createTrainVal(region, "$datapath/$cityname.h5", datapath, downsamplingDistort, 1_000_000, 10_000)  -->  createTrainVal(region, "$datapath/$cityname.h5", datapath, downsamplingDistort, 400_000, 5_000)

19°. Execute tdrive2h5.jl:
$ julia tdrive2h5.jl
Obs: As 532.511 trajetórias foram processadas e arquivo tdrive.h5 foi criado em /data!

20°. Execute preprocess.jl
$ julia preprocess.jl
Obs: Os arquivos de treino, validação, vocabulário e param-cels foram criados! 

$ cd ..

21°. Agora execute o treino:
$ python t2vec.py -vocab_size 19693 -criterion_name "KLDIV" -knearestvocabs "data/tdrive-vocab-dist-cell100.h5"
Obs: Ao final do treino, dentro do diretório data/, os arquivos checkpoint.pt e best_model.pt foram criados!

22°. Para obtenção dos resultados, copie o notebook t2vec.ipnyp para um novo arquivo Julia (note.jl) dentro do diretório experiments/. Após realizada a cópia, faça:
$ cd experiments/

23°. Em note.jl atualize as seguintes linhas:
 9  -->  "/home/jupyter-wilken.dantas@ufc.-af1ea/.conda/envs/tdrive/t2vec/data"
66  -->  "/home/jupyter-wilken.dantas@ufc.-af1ea/.conda/envs/tdrive/t2vec/"
68  -->  "/home/jupyter-wilken.dantas@ufc.-af1ea/.conda/envs/tdrive/t2vec/experiment"
63  -->  t2vec = `python t2vec.py -mode 2 -vocab_size 19693 -checkpoint $checkpoint -prefix $prefix`
47  -->  start = 400_000+20_000
48  -->  num_query = 500
49  -->  num_db = 50_000
137  -->  dbsizes = [10_000, 20_000, 30_000, 40_000, 50_000]

Obs: No diretório corrente há uma cópia do arquivo note.jl!

24°. Agora, em utils.jl atualize as seguintes linhas:
56 -->  min_length=5
57 -->  max_length=50

25°. Finalmente, execute o arquivo note.jl para obter os resultados!
$ julia note.jl
